{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thiwanka-Sandakalum/ETL-pipline/blob/main/Web_Crawlerv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhLLPvwh8mY"
      },
      "source": [
        "# üéì Academic Web Crawler - Clean Pipeline\n",
        "\n",
        "A modular web crawler for academic websites. Run each cell independently.\n",
        "\n",
        "## Pipeline Steps:\n",
        "1. **Setup** - Install packages and start Ollama\n",
        "2. **Configure** - Set your target URL and parameters\n",
        "3. **Discover URLs** - Crawl website and find all links\n",
        "4. **Filter URLs** - Use AI to select relevant academic pages\n",
        "5. **Download Content** - Get HTML content from filtered URLs\n",
        "6. **Extract Text** - Clean HTML and save to .txt files\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSvBeCDHh8mb"
      },
      "source": [
        "## üì¶ Step 1: Installation & Setup\n",
        "\n",
        "Install all required packages and set up Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st3Wgl-sh8mb"
      },
      "outputs": [],
      "source": [
        "# Install Python packages\n",
        "!pip install -q langchain-ollama beautifulsoup4 lxml requests tqdm\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A2hSqXGh8mc"
      },
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!sudo apt update > /dev/null 2>&1\n",
        "!sudo apt install -y pciutils > /dev/null 2>&1\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"‚úÖ Ollama installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLiKgNL3i-cn"
      },
      "outputs": [],
      "source": [
        "!ollama serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju5dJhYTh8mc"
      },
      "outputs": [],
      "source": [
        "# Download Llama 3.2 model\n",
        "!ollama pull llama3.2\n",
        "\n",
        "print(\"‚úÖ Llama 3.2 model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvuLsK4Wh8md"
      },
      "source": [
        "## üöÄ Step 2: Initialize Ollama Server & LLM\n",
        "\n",
        "Start the Ollama server and initialize the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7r6L3jMh8md"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "subprocess.run([\"pkill\", \"-9\", \"ollama\"], stderr=subprocess.DEVNULL)\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama server in background\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                     stdout=subprocess.DEVNULL,\n",
        "                     stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Start the server for the first time\n",
        "thread = threading.Thread(target=run_ollama_serve, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(5) # Give the server some time to start\n",
        "\n",
        "# Ensure the model is pulled after the server starts\n",
        "print(\"‚¨áÔ∏è Pulling Llama 3.2 model if not present...\")\n",
        "subprocess.run([\"ollama\", \"pull\", \"llama3.2\"],\n",
        "                 stdout=subprocess.DEVNULL,\n",
        "                 stderr=subprocess.DEVNULL)\n",
        "print(\"‚úÖ Llama 3.2 model pull initiated (or already present).\")\n",
        "\n",
        "\n",
        "# Initialize LLM with retry logic\n",
        "print(\"ü§ñ Initializing Llama 3.2...\")\n",
        "llm = None\n",
        "\n",
        "for attempt in range(3):\n",
        "    try:\n",
        "        llm = OllamaLLM(model=\"llama3.2\", temperature=0)\n",
        "        test_response = llm.invoke(\"Say OK\")\n",
        "        print(f\"‚úÖ LLM initialized successfully! Test response: {test_response}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if attempt < 2:\n",
        "            print(f\"‚ö†Ô∏è  Retry {attempt + 1}/3...\")\n",
        "            # Kill and restart server\n",
        "            subprocess.run([\"pkill\", \"-9\", \"ollama\"], stderr=subprocess.DEVNULL)\n",
        "            time.sleep(2)\n",
        "            threading.Thread(target=run_ollama_serve, daemon=True).start()\n",
        "            time.sleep(5)\n",
        "            # Re-attempt pulling the model\n",
        "            print(\"‚¨áÔ∏è Retrying Llama 3.2 model pull...\")\n",
        "            subprocess.run([\"ollama\", \"pull\", \"llama3.2\"],\n",
        "                             stdout=subprocess.DEVNULL,\n",
        "                             stderr=subprocess.DEVNULL)\n",
        "            time.sleep(2) # Give some time for pull\n",
        "        else:\n",
        "            raise Exception(f\"‚ùå Failed to initialize LLM: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DYaJkN9h8md"
      },
      "source": [
        "## ‚öôÔ∏è Step 3: Configuration\n",
        "\n",
        "Set your target URL and crawling parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94485672"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUIHZGUUh8md"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION - MODIFY THESE VALUES\n",
        "# ============================================================\n",
        "\n",
        "# Target website to crawl\n",
        "START_URL = \"https://mgmt.cmb.ac.lk/mgmt_department-of-accounting\"\n",
        "\n",
        "# Maximum crawl depth (0 = only start page, 1 = start + linked pages, etc.)\n",
        "MAX_DEPTH = 50\n",
        "MAX_WORKERS=100\n",
        "# Output directory for final text files\n",
        "# Automatically generate OUTPUT_DIR based on START_URL's domain\n",
        "parsed_url = urlparse(START_URL)\n",
        "domain = parsed_url.netloc.replace('www.', '').replace('.', '_')\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/academic_content_output/{domain}\"\n",
        "\n",
        "# Parallel processing settings\n",
        "MAX_DOWNLOAD_WORKERS = 10  # Concurrent downloads\n",
        "AI_BATCH_SIZE = 10         # URLs per AI batch\n",
        "AI_MAX_WORKERS = 5        # Concurrent AI requests\n",
        "\n",
        "# Minimum text length to save (characters)\n",
        "MIN_TEXT_LENGTH = 500\n",
        "\n",
        "print(\"‚úÖ Configuration set:\")\n",
        "print(f\"   Start URL: {START_URL}\")\n",
        "print(f\"   Max Depth: {MAX_DEPTH}\")\n",
        "print(f\"   Output Dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLQvT2WQh8me"
      },
      "source": [
        "## üï∑Ô∏è Step 4: URL Discovery\n",
        "\n",
        "Crawl the website and discover all URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2G6fET8h8me"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, urldefrag\n",
        "from typing import Set, List, Optional, Dict\n",
        "from collections import deque # Import deque\n",
        "import threading # Import threading for locks\n",
        "from queue import Queue # Import Queue\n",
        "from concurrent.futures import ThreadPoolExecutor # Import ThreadPoolExecutor\n",
        "MAX_WORKERS=20\n",
        "\n",
        "# Keywords to reject during crawling\n",
        "REJECT_KEYWORDS = set([\n",
        "    \"assets\", \"attachments\", \"audio\", \"css\", \"downloads\", \"favicon\", \"fonts\", \"images\", \"img\", \"js\", \"media\", \"misc\", \"pdf\", \"photo\", \"pict\", \"png\", \"scripts\", \"static\", \"styles\", \"themes\", \"uploads\", \"video\", \"wp-content\", \"wp-includes\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".mp4\", \".mp3\", \".zip\", \".tar\", \".gz\",\n",
        "    # authentication / portals\n",
        "    \"account\", \"auth\", \"authenticate\", \"authentication\", \"cas\", \"dashboard\", \"ezproxy\", \"forgot\", \"identity\", \"login\", \"logout\", \"mfa\", \"my-account\", \"my-profile\", \"netid\", \"password\", \"portal\", \"proxy\", \"register\", \"saml\", \"shibboleth\", \"signin\", \"signout\", \"signup\", \"sso\", \"user\", \"validate\",\n",
        "    # news / marketing / media\n",
        "    \"announcement\", \"archive\", \"blog\", \"calendar\", \"category\", \"event\", \"events\", \"feed\", \"gallery\", \"magazine\", \"news\", \"newsletter\", \"press\", \"rss\", \"schedule\", \"slideshow\", \"stories\", \"tags\", \"upcoming\", \"view-event\",\n",
        "    # careers / jobs\n",
        "    \"applicant\", \"benefits\", \"career\", \"careers\", \"compensation\", \"employment\", \"hiring\", \"hr\", \"human-resources\", \"internship\", \"job\", \"jobs\", \"onboarding\", \"opportunities\", \"payroll\", \"position\", \"recruitment\", \"staff-training\", \"vacancy\", \"vacancies\",\n",
        "    # legal / policy pages\n",
        "    \"accessibility\", \"ada\", \"compliance\", \"cookie\", \"cookies\", \"copyright\", \"disclaimer\", \"legal\", \"license\", \"maintainer\", \"maintenance\", \"policy\", \"privacy\", \"security\", \"terms\", \"terms-and-conditions\",\n",
        "    # social media / external platforms\n",
        "    \"facebook\", \"instagram\", \"linkedin\", \"pinterest\", \"share\", \"snapchat\", \"tiktok\", \"tumblr\", \"twitter\", \"vimeo\", \"whatsapp\", \"youtube\",\n",
        "    # tracking / analytics\n",
        "    \"analytics\", \"fbclid\", \"ga_\", \"gclid\", \"google-analytics\", \"log\", \"logs\", \"metrics\", \"pixel\", \"stats\", \"tracker\", \"tracking\", \"utm_\",\n",
        "    # search / filters / pagination\n",
        "    \"filter\", \"limit\", \"offset\", \"order\", \"page\", \"query\", \"results\", \"search\", \"sort\", \"view\", \"viewitems\",\n",
        "    \"tag/\", \"tags/\",\n",
        "    \"category/\", \"categories/\",\n",
        "    \"author/\",\n",
        "    \"page/\",          # pagination\n",
        "    \"/202\",\n",
        "    # system / administrative\n",
        "    \"admin\", \"api\", \"backup\", \"bin\", \"cache\", \"cgi-bin\", \"config\", \"configuration\", \"cron\", \"devel\", \"dev\", \"etc\", \"install\", \"modules\", \"node/add\", \"php\", \"plugins\", \"server-status\", \"settings\", \"sql\", \"structure\", \"tmp\", \"update\", \"upgrade\", \"var\", \"wp-admin\", \"xmlrpc\"\n",
        "])\n",
        "\n",
        "# File extensions to skip\n",
        "REJECT_EXTENSIONS = (\n",
        "    \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".webp\",\n",
        "    \".css\", \".js\", \".map\", \".pdf\", \".zip\", \".rar\",\n",
        "    \".mp4\", \".mp3\", \".doc\", \".docx\", \".xls\", \".xlsx\"\n",
        ")\n",
        "\n",
        "# ================= GLOBAL STATE =================\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
        "\n",
        "visited: Set[str] = set()\n",
        "program_tree: Dict[str, List[str]] = {}\n",
        "\n",
        "visited_lock = threading.Lock()\n",
        "tree_lock = threading.Lock()\n",
        "\n",
        "task_queue = Queue()\n",
        "\n",
        "# ================= UTILITIES =================\n",
        "def normalize(url: str) -> str | None:\n",
        "    url, _ = urldefrag(url)\n",
        "    parsed = urlparse(url)\n",
        "    if parsed.scheme not in (\"http\", \"https\"):\n",
        "        return None\n",
        "    return parsed._replace(query=\"\").geturl().rstrip(\"/\")\n",
        "\n",
        "def is_crawlable_link(url: str, base_domain: str) -> bool:\n",
        "    parsed = urlparse(url)\n",
        "    if parsed.netloc != base_domain:\n",
        "        return False\n",
        "    path = parsed.path.lower()\n",
        "    if any(k in path for k in REJECT_KEYWORDS):\n",
        "        return False\n",
        "    if any(path.endswith(ext) for ext in REJECT_EXTENSIONS):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# ================= EXTRACTION =================\n",
        "def extract_program_links(page_url: str, base_domain: str) -> List[str]:\n",
        "    try:\n",
        "        r = session.get(page_url, timeout=10)\n",
        "        if r.status_code != 200:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "        links = []\n",
        "\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            full = normalize(urljoin(page_url, a[\"href\"]))\n",
        "            if not full:\n",
        "                continue\n",
        "\n",
        "            with visited_lock:\n",
        "                if full in visited:\n",
        "                    continue\n",
        "\n",
        "            if is_crawlable_link(full, base_domain):\n",
        "                links.append(full)\n",
        "\n",
        "        return list(dict.fromkeys(links))  # preserve order\n",
        "    except requests.RequestException:\n",
        "        return []\n",
        "\n",
        "# ================= WORKER =================\n",
        "def worker(base_domain: str):\n",
        "    while True:\n",
        "        try:\n",
        "            url, depth = task_queue.get(timeout=2)\n",
        "        except:\n",
        "            return  # Queue empty ‚Üí exit worker\n",
        "\n",
        "        with visited_lock:\n",
        "            if url in visited:\n",
        "                task_queue.task_done()\n",
        "                continue\n",
        "            visited.add(url)\n",
        "\n",
        "        print(f\"[Depth {depth}] {url}\")\n",
        "\n",
        "        if depth < MAX_DEPTH:\n",
        "            children = extract_program_links(url, base_domain)\n",
        "\n",
        "            with tree_lock:\n",
        "                program_tree[url] = children\n",
        "\n",
        "            for child in children:\n",
        "                task_queue.put((child, depth + 1))\n",
        "\n",
        "        task_queue.task_done()\n",
        "\n",
        "# ================= RUNNER =================\n",
        "def run_crawler_and_get_urls() -> List[str]:\n",
        "    start = normalize(START_URL)\n",
        "    if not start:\n",
        "        return []\n",
        "\n",
        "    base_domain = urlparse(start).netloc\n",
        "    task_queue.put((start, 0))\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        print(\"max workers\",MAX_WORKERS)\n",
        "        for _ in range(MAX_WORKERS):\n",
        "            executor.submit(worker, base_domain)\n",
        "\n",
        "        task_queue.join()  # Wait until queue is empty\n",
        "\n",
        "    return sorted(visited)\n",
        "\n",
        "# ================= EXECUTION =================\n",
        "if __name__ == \"__main__\":\n",
        "    discovered_urls = run_crawler_and_get_urls()\n",
        "\n",
        "    print(\"\\nDiscovered URLs (first 20):\")\n",
        "    for u in discovered_urls[:20]:\n",
        "        print(u)\n",
        "\n",
        "    if len(discovered_urls) > 20:\n",
        "        print(f\"... and {len(discovered_urls) - 20} more\")\n",
        "\n",
        "    # Optional: print program tree structure\n",
        "    print(\"\\nProgram tree (sample):\")\n",
        "    for parent, children in list(program_tree.items())[:10]:\n",
        "        print(f\"{parent}\")\n",
        "        for c in children:\n",
        "            print(f\"  ‚îî‚îÄ {c}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nProgram tree (sample):\")\n",
        "for parent, children in list(program_tree.items()):\n",
        "        print(f\"{parent}\")\n",
        "        for c in children:\n",
        "            print(f\"  ‚îî‚îÄ {c}\")"
      ],
      "metadata": {
        "id": "XlBMS3LPMrq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCQUNYVdh8me"
      },
      "source": [
        "## üîç Step 5: Filter URLs with AI\n",
        "\n",
        "Use AI and heuristics to select academically relevant URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_l5FiO34h8mf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "def ai_filter_batch(urls_batch: list, llm) -> list:\n",
        "    \"\"\"Use LLM to filter a batch of URLs with improved prompting.\"\"\"\n",
        "    urls_text = \"\\n\".join(f\"- {url}\" for url in urls_batch)\n",
        "\n",
        "    prompt = f\"\"\"You are a specialized URL classifier for university websites. Your ONLY task is to identify URLs that contain information about academic programs, courses, and degrees.\n",
        "\n",
        "**INCLUDE URLs that contain:**\n",
        "- Program listings (undergraduate/postgraduate programs)\n",
        "- Degree information (BSc, MSc, PhD, etc.)\n",
        "- Course catalogs or course unit descriptions\n",
        "- Department program pages (what programs a department offers)\n",
        "- Admission requirements for academic programs\n",
        "- Curriculum or syllabus details for degrees\n",
        "- Academic prospectus pages\n",
        "\n",
        "**EXCLUDE URLs about:**\n",
        "- Individual staff profiles or staff listings\n",
        "- Research projects, publications, or research centers\n",
        "- News articles, events, or announcements\n",
        "- Student societies, clubs, or social activities\n",
        "- Field trips, workshops, or seminars\n",
        "- Facilities, museums, libraries, or laboratories\n",
        "- Alumni information or past students\n",
        "- Awards, scholarships (unless part of program description)\n",
        "- Contact pages, history pages, or \"about us\" pages\n",
        "- Committee information or administrative details\n",
        "- Date-based URLs (e.g., /2018/05/23/)\n",
        "- Image attachments or gallery pages\n",
        "\n",
        "**EXAMPLES:**\n",
        "‚úì INCLUDE: /undergraduate-courses, /postgraduate, /bsc-special-degree, /mat/program\n",
        "‚úó EXCLUDE: /staff, /academic-staff, /publications, /news, /alumni, /contact\n",
        "\n",
        "Analyze these URLs and return ONLY those that describe academic programs or courses.\n",
        "\n",
        "URLs to evaluate:\n",
        "{urls_text}\n",
        "\n",
        "Return your response as a JSON array of selected URLs. If none qualify, return [].\n",
        "\n",
        "JSON array:\"\"\"\n",
        "\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = llm.invoke(prompt).strip()\n",
        "\n",
        "            # Extract JSON array\n",
        "            match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
        "            if match:\n",
        "                selected = json.loads(match.group())\n",
        "                return [url for url in selected if url in urls_batch]\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  AI filter: No JSON array in response (attempt {attempt+1}/{max_retries})\")\n",
        "                break\n",
        "        except Exception as e:\n",
        "            error_message = str(e).lower()\n",
        "            if \"connection refused\" in error_message or isinstance(e, ConnectionRefusedError):\n",
        "                print(f\"‚ö†Ô∏è  Connection error: {e} (attempt {attempt+1}/{max_retries})\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(2 * (attempt + 1))\n",
        "                    # Restart Ollama server\n",
        "                    print(\"Restarting Ollama server...\")\n",
        "                    subprocess.run([\"pkill\", \"-9\", \"ollama\"], stderr=subprocess.DEVNULL)\n",
        "                    time.sleep(2)\n",
        "                    subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                                   stdout=subprocess.DEVNULL,\n",
        "                                   stderr=subprocess.DEVNULL)\n",
        "                    time.sleep(5)\n",
        "                    subprocess.run([\"ollama\", \"pull\", \"llama3.2\"],\n",
        "                                 stdout=subprocess.DEVNULL,\n",
        "                                 stderr=subprocess.DEVNULL)\n",
        "                    time.sleep(2)\n",
        "                else:\n",
        "                    print(f\"‚ùå Failed after {max_retries} attempts\")\n",
        "                    break\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  General error: {e} (attempt {attempt+1}/{max_retries})\")\n",
        "                break\n",
        "\n",
        "    return []\n",
        "\n",
        "def filter_urls_with_ai(urls: list, llm, batch_size: int, max_workers: int) -> list:\n",
        "    \"\"\"Filter URLs using AI processing with progress tracking.\"\"\"\n",
        "    print(f\"üîç Filtering {len(urls)} URLs using AI...\\n\")\n",
        "\n",
        "    batches = [urls[i:i + batch_size]\n",
        "               for i in range(0, len(urls), batch_size)]\n",
        "\n",
        "    ai_filtered = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(ai_filter_batch, batch, llm): i\n",
        "                  for i, batch in enumerate(batches)}\n",
        "\n",
        "        for future in tqdm(as_completed(futures),\n",
        "                          total=len(futures),\n",
        "                          desc=\"   Processing batches\"):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    ai_filtered.extend(result)\n",
        "                time.sleep(0.5)  # Rate limiting\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Batch processing error: {e}\")\n",
        "\n",
        "    # Deduplicate and sort\n",
        "    final = sorted(list(set(ai_filtered)))\n",
        "\n",
        "    print(f\"\\n‚úÖ AI Filtering complete!\")\n",
        "    print(f\"   Final relevant URLs: {len(final)}\")\n",
        "    return final\n",
        "\n",
        "# Example usage (assuming you have your discovered_urls and llm setup):\n",
        "# AI_BATCH_SIZE = 10\n",
        "# AI_MAX_WORKERS = 3\n",
        "#\n",
        "filtered_urls = filter_urls_with_ai(\n",
        "    discovered_urls,\n",
        "    llm,\n",
        "    AI_BATCH_SIZE,\n",
        "    AI_MAX_WORKERS\n",
        ")\n",
        "\n",
        "print(\"\\nüìã Filtered Program URLs:\")\n",
        "for url in filtered_urls[:20]:\n",
        "    print(f\"   {url}\")\n",
        "if len(filtered_urls) > 20:\n",
        "    print(f\"   ... and {len(filtered_urls) - 20} more\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìã Filtered Program URLs:\")\n",
        "for url in filtered_urls:\n",
        "    print(f\"   {url}\")"
      ],
      "metadata": {
        "id": "9bfx1h0KJ04Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utOtc6Txh8mf"
      },
      "source": [
        "## üì• Step 6: Download Content\n",
        "\n",
        "Download HTML content from all filtered URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW0ufqbWh8mf"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Optional\n",
        "\n",
        "def download_url(url: str) -> Optional[str]:\n",
        "    \"\"\"Download HTML content from URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            url,\n",
        "            timeout=15,\n",
        "            headers={\"User-Agent\": \"Mozilla/5.0 (Academic Crawler)\"}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_all_content(urls: List[str], max_workers: int) -> Dict[str, str]:\n",
        "    \"\"\"Download HTML content from all URLs in parallel.\"\"\"\n",
        "    print(f\"üì• Downloading content from {len(urls)} URLs...\\n\")\n",
        "\n",
        "    content_map = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(download_url, url): url for url in urls}\n",
        "\n",
        "        for future in tqdm(as_completed(futures),\n",
        "                          total=len(futures),\n",
        "                          desc=\"   Downloading\"):\n",
        "            url = futures[future]\n",
        "            try:\n",
        "                html = future.result()\n",
        "                if html:\n",
        "                    content_map[url] = html\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error processing {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Download complete!\")\n",
        "    print(f\"   Successfully downloaded: {len(content_map)}/{len(urls)} pages\")\n",
        "    return content_map\n",
        "\n",
        "# Download all content\n",
        "downloaded_content = download_all_content(filtered_urls, MAX_DOWNLOAD_WORKERS)\n",
        "\n",
        "print(f\"\\nüìä Content statistics:\")\n",
        "total_size = sum(len(html) for html in downloaded_content.values())\n",
        "print(f\"   Total HTML size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "print(f\"   Average page size: {total_size / len(downloaded_content) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzGIeCX_h8mg"
      },
      "source": [
        "## üìù Step 7: Extract Text & Save Files\n",
        "\n",
        "Extract clean text from HTML and save to .txt files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkYvNQath8mg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def clean_html_to_text(html: str) -> str:\n",
        "    \"\"\"Extract clean text from HTML.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Remove unwanted elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\",\n",
        "                     \"aside\", \"header\", \"iframe\", \"form\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    # Remove comments\n",
        "    for comment in soup.find_all(string=lambda text:\n",
        "                                 isinstance(text, str) and\n",
        "                                 text.strip().startswith(\"<!--\")):\n",
        "        comment.extract()\n",
        "\n",
        "    # Extract text\n",
        "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "    # Clean up whitespace\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    clean_text = \"\\n\".join(lines)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def safe_filename(url: str) -> str:\n",
        "    \"\"\"Generate safe filename from URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    path = parsed.path if parsed.path else \"root\"\n",
        "\n",
        "    # Create readable filename\n",
        "    filename = f\"{parsed.netloc}{path}\"\n",
        "    filename = filename.replace(\"/\", \"_\").replace(\"?\", \"_\")\n",
        "    filename = filename.replace(\"&\", \"_\").replace(\":\", \"_\")\n",
        "    filename = filename.replace(\"=\", \"_\").replace(\".\", \"_\")\n",
        "\n",
        "    # Limit length\n",
        "    if len(filename) > 150:\n",
        "        filename = filename[:150]\n",
        "\n",
        "    return filename + \".txt\"\n",
        "\n",
        "def save_content_to_files(\n",
        "    content_map: Dict[str, str],\n",
        "    output_dir: str,\n",
        "    min_length: int\n",
        ") -> int:\n",
        "    \"\"\"Extract text and save to files.\"\"\"\n",
        "    print(f\"üìù Extracting text and saving files...\\n\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    saved_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for url, html in tqdm(content_map.items(), desc=\"   Processing\"):\n",
        "        try:\n",
        "            # Extract clean text\n",
        "            clean_text = clean_html_to_text(html)\n",
        "\n",
        "            # Skip if too short\n",
        "            if len(clean_text) < min_length:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Generate filename\n",
        "            filename = safe_filename(url)\n",
        "            filepath = output_path / filename\n",
        "\n",
        "            # Write to file\n",
        "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "                f.write(clean_text)\n",
        "\n",
        "            saved_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error processing {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Text extraction complete!\")\n",
        "    print(f\"   Files saved: {saved_count}\")\n",
        "    print(f\"   Files skipped (too short): {skipped_count}\")\n",
        "    print(f\"   Output directory: {output_dir}\")\n",
        "\n",
        "    return saved_count\n",
        "\n",
        "# Extract and save all content\n",
        "saved_files = save_content_to_files(\n",
        "    downloaded_content,\n",
        "    OUTPUT_DIR,\n",
        "    MIN_TEXT_LENGTH\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}