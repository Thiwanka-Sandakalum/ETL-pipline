{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhLLPvwh8mY"
      },
      "source": [
        "# üéì Academic Web Crawler - Clean Pipeline\n",
        "\n",
        "A modular web crawler for academic websites. Run each cell independently.\n",
        "\n",
        "## Pipeline Steps:\n",
        "1. **Setup** - Install packages and start Ollama\n",
        "2. **Configure** - Set your target URL and parameters\n",
        "3. **Discover URLs** - Crawl website and find all links\n",
        "4. **Filter URLs** - Use AI to select relevant academic pages\n",
        "5. **Download Content** - Get HTML content from filtered URLs\n",
        "6. **Extract Text** - Clean HTML and save to .txt files\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSvBeCDHh8mb"
      },
      "source": [
        "## üì¶ Step 1: Installation & Setup\n",
        "\n",
        "Install all required packages and set up Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st3Wgl-sh8mb",
        "outputId": "9bb7e2f8-35fa-45d3-ee52-bc0fcfba2739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install Python packages\n",
        "!pip install -q langchain-ollama beautifulsoup4 lxml requests tqdm\n",
        "\n",
        "print(\"‚úÖ Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLiKgNL3i-cn",
        "outputId": "fa203171-dcff-4abf-a5a6-3b590c3b657c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
          ]
        }
      ],
      "source": [
        "!ollama serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A2hSqXGh8mc",
        "outputId": "40b28cd0-0601-43d7-defc-2831d0094a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "‚úÖ Ollama installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install Ollama\n",
        "!sudo apt update > /dev/null 2>&1\n",
        "!sudo apt install -y pciutils > /dev/null 2>&1\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"‚úÖ Ollama installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju5dJhYTh8mc",
        "outputId": "5c4d1715-3bc8-47ec-ef68-e11717532590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "‚úÖ Llama 3.2 model ready!\n"
          ]
        }
      ],
      "source": [
        "# Download Llama 3.2 model\n",
        "!ollama pull llama3.2\n",
        "\n",
        "print(\"‚úÖ Llama 3.2 model ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvuLsK4Wh8md"
      },
      "source": [
        "## üöÄ Step 2: Initialize Ollama Server & LLM\n",
        "\n",
        "Start the Ollama server and initialize the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7r6L3jMh8md",
        "outputId": "aebcee93-dc80-4d0b-9168-b4cfc5b202b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Ollama server...\n",
            "ü§ñ Initializing Llama 3.2...\n",
            "‚úÖ LLM initialized successfully! Test response: OK\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "subprocess.run([\"pkill\", \"-9\", \"ollama\"], stderr=subprocess.DEVNULL)\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Ollama server in background\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                     stdout=subprocess.DEVNULL,\n",
        "                     stderr=subprocess.DEVNULL)\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve, daemon=True)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "\n",
        "# Initialize LLM with retry logic\n",
        "print(\"ü§ñ Initializing Llama 3.2...\")\n",
        "llm = None\n",
        "\n",
        "for attempt in range(3):\n",
        "    try:\n",
        "        llm = OllamaLLM(model=\"llama3.2\", temperature=0)\n",
        "        test_response = llm.invoke(\"Say OK\")\n",
        "        print(f\"‚úÖ LLM initialized successfully! Test response: {test_response}\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if attempt < 2:\n",
        "            print(f\"‚ö†Ô∏è  Retry {attempt + 1}/3...\")\n",
        "            subprocess.run([\"pkill\", \"-9\", \"ollama\"], stderr=subprocess.DEVNULL)\n",
        "            time.sleep(2)\n",
        "            threading.Thread(target=run_ollama_serve, daemon=True).start()\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            raise Exception(f\"‚ùå Failed to initialize LLM: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DYaJkN9h8md"
      },
      "source": [
        "## ‚öôÔ∏è Step 3: Configuration\n",
        "\n",
        "Set your target URL and crawling parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94485672",
        "outputId": "56f6456f-83a6-4c68-999f-55a7abaaf800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUIHZGUUh8md",
        "outputId": "df5bb5ce-aae8-4957-aabd-97de069dccda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration set:\n",
            "   Start URL: https://www.bms.ac.lk/\n",
            "   Max Depth: 100\n",
            "   Output Dir: /content/drive/MyDrive/academic_content_output/bms_ac_lk\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - MODIFY THESE VALUES\n",
        "# ============================================================\n",
        "\n",
        "# Target website to crawl\n",
        "START_URL = \"https://www.bms.ac.lk/\"\n",
        "\n",
        "# Maximum crawl depth (0 = only start page, 1 = start + linked pages, etc.)\n",
        "MAX_DEPTH = 100\n",
        "\n",
        "# Output directory for final text files\n",
        "# Automatically generate OUTPUT_DIR based on START_URL's domain\n",
        "parsed_url = urlparse(START_URL)\n",
        "domain = parsed_url.netloc.replace('www.', '').replace('.', '_')\n",
        "OUTPUT_DIR = f\"/content/drive/MyDrive/academic_content_output/{domain}\"\n",
        "\n",
        "# Parallel processing settings\n",
        "MAX_DOWNLOAD_WORKERS = 32  # Concurrent downloads\n",
        "AI_BATCH_SIZE = 20         # URLs per AI batch\n",
        "AI_MAX_WORKERS = 10        # Concurrent AI requests\n",
        "\n",
        "# Minimum text length to save (characters)\n",
        "MIN_TEXT_LENGTH = 500\n",
        "\n",
        "print(\"‚úÖ Configuration set:\")\n",
        "print(f\"   Start URL: {START_URL}\")\n",
        "print(f\"   Max Depth: {MAX_DEPTH}\")\n",
        "print(f\"   Output Dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLQvT2WQh8me"
      },
      "source": [
        "## üï∑Ô∏è Step 4: URL Discovery\n",
        "\n",
        "Crawl the website and discover all URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2G6fET8h8me",
        "outputId": "aeacde3d-fd1b-4c31-b7c3-924e909b9c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üï∑Ô∏è  Starting crawl: https://www.bms.ac.lk/\n",
            "   Max depth: 100\n",
            "\n",
            "[Depth 0] Crawling: https://www.bms.ac.lk/\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 1] Crawling: https://www.bms.ac.lk/Executive-Certificate-in-Management\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 2] Crawling: https://www.bms.ac.lk/MBA-Digital-Transformation\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 3] Crawling: https://www.bms.ac.lk/HD-Biomedical-Science\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 4] Crawling: https://www.bms.ac.lk/HD-Food-Science-and-Nutrition\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 5] Crawling: https://www.bms.ac.lk/About-BMS\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 6] Crawling: https://www.bms.ac.lk/EARBio/index.html\n",
            "           Found: 0 links\n",
            "\n",
            "[Depth 6] Crawling: https://www.bms.ac.lk/Higher-National-Diploma\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 7] Crawling: https://www.bms.ac.lk/MBA-Teesside\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 8] Crawling: https://www.bms.ac.lk/Board-of-Governors\n",
            "           Found: 30 links\n",
            "\n",
            "[Depth 9] Crawling: https://www.bms.ac.lk/International-Foundation-Diploma-(Business)\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 10] Crawling: https://www.bms.ac.lk/msc-cancer-and-molecular-diagnostic\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 11] Crawling: https://www.bms.ac.lk/MSc-Northumbria\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 12] Crawling: https://www.bms.ac.lk/BSc-(Hons)-Biomedical-Science\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 13] Crawling: https://www.bms.ac.lk/index\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 14] Crawling: https://www.bms.ac.lk/MSc-Digital-Marketing\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 15] Crawling: https://www.bms.ac.lk/Senior-Leadership-Team\n",
            "           Found: 30 links\n",
            "\n",
            "[Depth 16] Crawling: https://www.bms.ac.lk/Organizational-Chart\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 17] Crawling: https://www.bms.ac.lk/cdn-cgi/l/email-protection\n",
            "           Found: 0 links\n",
            "\n",
            "[Depth 17] Crawling: https://www.bms.ac.lk/bsc-hons-global-business-management\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 18] Crawling: https://www.bms.ac.lk/Council-of-Management\n",
            "           Found: 30 links\n",
            "\n",
            "[Depth 19] Crawling: https://www.bms.ac.lk/bsc-hons-global-business-management-marketing\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 20] Crawling: https://www.bms.ac.lk/EAR2018/index.html\n",
            "           Found: 0 links\n",
            "\n",
            "[Depth 20] Crawling: https://www.bms.ac.lk/Executive-Diploma-in-Management\n",
            "           Found: 30 links\n",
            "\n",
            "[Depth 21] Crawling: https://www.bms.ac.lk/Contact-US.html\n",
            "           Found: 1 links\n",
            "\n",
            "[Depth 22] Crawling: https://www.bms.ac.lk/contact\n",
            "           Found: 1 links\n",
            "\n",
            "[Depth 21] Crawling: https://www.bms.ac.lk/Bachelor-of-Business-Management(Hons)\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 22] Crawling: https://www.bms.ac.lk/Contact-US\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 23] Crawling: https://www.bms.ac.lk/BSc-(Hons)-Food-Science-and-Nutrition\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 24] Crawling: https://www.bms.ac.lk/International-Foundation-Diploma-(Applied-Science)\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 25] Crawling: https://www.bms.ac.lk/Graduate-Diploma-In-Management\n",
            "           Found: 29 links\n",
            "\n",
            "[Depth 26] Crawling: https://www.bms.ac.lk/Academic-Senate\n",
            "           Found: 30 links\n",
            "\n",
            "[Depth 27] Crawling: https://www.bms.ac.lk/index.html\n",
            "           Found: 1 links\n",
            "\n",
            "[Depth 17] Crawling: https://www.bms.ac.lk/EAR2018/index\n",
            "           Found: 1 links\n",
            "\n",
            "[Depth 17] Crawling: https://www.bms.ac.lk/EARBio/index\n",
            "           Found: 1 links\n",
            "\n",
            "\n",
            "‚úÖ Crawl complete!\n",
            "   Total URLs discovered: 34\n",
            "\n",
            "üìã Sample URLs:\n",
            "   https://www.bms.ac.lk/About-BMS\n",
            "   https://www.bms.ac.lk/Academic-Senate\n",
            "   https://www.bms.ac.lk/BSc-(Hons)-Biomedical-Science\n",
            "   https://www.bms.ac.lk/BSc-(Hons)-Food-Science-and-Nutrition\n",
            "   https://www.bms.ac.lk/Bachelor-of-Business-Management(Hons)\n",
            "   https://www.bms.ac.lk/Board-of-Governors\n",
            "   https://www.bms.ac.lk/Contact-US\n",
            "   https://www.bms.ac.lk/Contact-US.html\n",
            "   https://www.bms.ac.lk/Council-of-Management\n",
            "   https://www.bms.ac.lk/EAR2018/index\n",
            "   ... and 24 more\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse, urldefrag\n",
        "from typing import Set, List\n",
        "\n",
        "# Keywords to reject during crawling\n",
        "REJECT_KEYWORDS = [\n",
        "    \"assets\", \"attachments\", \"audio\", \"css\", \"downloads\", \"favicon\", \"fonts\", \"images\", \"img\", \"js\", \"media\", \"misc\", \"pdf\", \"photo\", \"pict\", \"png\", \"scripts\", \"static\", \"styles\", \"themes\", \"uploads\", \"video\", \"wp-content\", \"wp-includes\", \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".mp4\", \".mp3\", \".zip\", \".tar\", \".gz\",\n",
        "    # authentication / portals\n",
        "    \"account\", \"auth\", \"authenticate\", \"authentication\", \"cas\", \"dashboard\", \"ezproxy\", \"forgot\", \"identity\", \"login\", \"logout\", \"mfa\", \"my-account\", \"my-profile\", \"netid\", \"password\", \"portal\", \"proxy\", \"register\", \"saml\", \"shibboleth\", \"signin\", \"signout\", \"signup\", \"sso\", \"user\", \"validate\",\n",
        "    # news / marketing / media\n",
        "    \"announcement\", \"archive\", \"blog\", \"calendar\", \"category\", \"event\", \"events\", \"feed\", \"gallery\", \"magazine\", \"news\", \"newsletter\", \"press\", \"rss\", \"schedule\", \"slideshow\", \"stories\", \"tags\", \"upcoming\", \"view-event\",\n",
        "    # careers / jobs\n",
        "    \"applicant\", \"benefits\", \"career\", \"careers\", \"compensation\", \"employment\", \"hiring\", \"hr\", \"human-resources\", \"internship\", \"job\", \"jobs\", \"onboarding\", \"opportunities\", \"payroll\", \"position\", \"recruitment\", \"staff-training\", \"vacancy\", \"vacancies\",\n",
        "    # legal / policy pages\n",
        "    \"accessibility\", \"ada\", \"compliance\", \"cookie\", \"cookies\", \"copyright\", \"disclaimer\", \"legal\", \"license\", \"maintainer\", \"maintenance\", \"policy\", \"privacy\", \"security\", \"terms\", \"terms-and-conditions\",\n",
        "    # social media / external platforms\n",
        "    \"facebook\", \"instagram\", \"linkedin\", \"pinterest\", \"share\", \"snapchat\", \"tiktok\", \"tumblr\", \"twitter\", \"vimeo\", \"whatsapp\", \"youtube\",\n",
        "    # tracking / analytics\n",
        "    \"analytics\", \"fbclid\", \"ga_\", \"gclid\", \"google-analytics\", \"log\", \"logs\", \"metrics\", \"pixel\", \"stats\", \"tracker\", \"tracking\", \"utm_\",\n",
        "    # search / filters / pagination\n",
        "    \"filter\", \"limit\", \"offset\", \"order\", \"page\", \"query\", \"results\", \"search\", \"sort\", \"view\", \"viewitems\",\n",
        "    # system / administrative\n",
        "    \"admin\", \"api\", \"backup\", \"bin\", \"cache\", \"cgi-bin\", \"config\", \"configuration\", \"cron\", \"devel\", \"dev\", \"etc\", \"install\", \"modules\", \"node/add\", \"php\", \"plugins\", \"server-status\", \"settings\", \"sql\", \"structure\", \"tmp\", \"update\", \"upgrade\", \"var\", \"wp-admin\", \"xmlrpc\"\n",
        "]\n",
        "\n",
        "# File extensions to skip\n",
        "REJECT_EXTENSIONS = [\n",
        "    \".jpg\", \".jpeg\", \".png\", \".gif\", \".svg\", \".webp\",\n",
        "    \".css\", \".js\", \".map\", \".pdf\", \".zip\", \".rar\",\n",
        "    \".mp4\", \".mp3\", \".doc\", \".docx\", \".xls\", \".xlsx\"\n",
        "]\n",
        "\n",
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Clean and normalize URL.\"\"\"\n",
        "    url, _ = urldefrag(url)  # Remove fragments (#section)\n",
        "    parsed = urlparse(url)\n",
        "\n",
        "    if parsed.scheme not in (\"http\", \"https\"):\n",
        "        return None\n",
        "\n",
        "    # Remove query parameters\n",
        "    clean = parsed._replace(query=\"\").geturl()\n",
        "\n",
        "    # Remove trailing slash\n",
        "    return clean.rstrip(\"/\")\n",
        "\n",
        "def is_useful_url(url: str, base_domain: str) -> bool:\n",
        "    \"\"\"Check if URL is worth crawling.\"\"\"\n",
        "    url_lower = url.lower()\n",
        "\n",
        "    # Must be same domain\n",
        "    if base_domain not in url_lower:\n",
        "        return False\n",
        "\n",
        "    # Skip file extensions\n",
        "    if any(url_lower.endswith(ext) for ext in REJECT_EXTENSIONS):\n",
        "        return False\n",
        "\n",
        "    # Skip rejected keywords\n",
        "    if any(keyword in url_lower for keyword in REJECT_KEYWORDS):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def get_links_from_page(url: str, start_url: str, base_domain: str) -> Set[str]:\n",
        "    \"\"\"Extract all valid links from a page.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10,\n",
        "                               headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "\n",
        "        links = set()\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = str(a_tag['href']).strip()\n",
        "\n",
        "            # Join with base URL\n",
        "            full_url = urljoin(start_url, href)\n",
        "\n",
        "            # Normalize\n",
        "            normalized = normalize_url(full_url)\n",
        "\n",
        "            # Validate\n",
        "            if (normalized and\n",
        "                normalized.startswith(start_url) and\n",
        "                is_useful_url(normalized, base_domain)):\n",
        "                links.add(normalized)\n",
        "\n",
        "        return links\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error fetching {url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "def crawl_website(start_url: str, max_depth: int) -> List[str]:\n",
        "    \"\"\"Crawl website and return all discovered URLs.\"\"\"\n",
        "    print(f\"üï∑Ô∏è  Starting crawl: {start_url}\")\n",
        "    print(f\"   Max depth: {max_depth}\\n\")\n",
        "\n",
        "    base_domain = urlparse(start_url).netloc\n",
        "    visited = set()\n",
        "    all_urls = set()\n",
        "\n",
        "    def _crawl_recursive(current_url: str, depth: int):\n",
        "        if depth > max_depth or current_url in visited:\n",
        "            return\n",
        "\n",
        "        visited.add(current_url)\n",
        "        print(f\"[Depth {depth}] Crawling: {current_url}\")\n",
        "\n",
        "        # Get links from current page\n",
        "        links = get_links_from_page(current_url, start_url, base_domain)\n",
        "        all_urls.update(links)\n",
        "\n",
        "        print(f\"           Found: {len(links)} links\\n\")\n",
        "\n",
        "        # Recursively crawl discovered links\n",
        "        for link in links:\n",
        "            _crawl_recursive(link, depth + 1)\n",
        "\n",
        "    # Start crawling\n",
        "    _crawl_recursive(start_url, 0)\n",
        "\n",
        "    result = sorted(list(all_urls))\n",
        "    print(f\"\\n‚úÖ Crawl complete!\")\n",
        "    print(f\"   Total URLs discovered: {len(result)}\")\n",
        "    return result\n",
        "\n",
        "# Run the crawler\n",
        "discovered_urls = crawl_website(START_URL, MAX_DEPTH)\n",
        "\n",
        "# Display first 10 URLs\n",
        "print(\"\\nüìã Sample URLs:\")\n",
        "for url in discovered_urls[:10]:\n",
        "    print(f\"   {url}\")\n",
        "if len(discovered_urls) > 10:\n",
        "    print(f\"   ... and {len(discovered_urls) - 10} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCQUNYVdh8me"
      },
      "source": [
        "## üîç Step 5: Filter URLs with AI\n",
        "\n",
        "Use AI and heuristics to select academically relevant URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l5FiO34h8mf",
        "outputId": "b9186530-fc81-463b-c34d-0e69b51f958d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Filtering 34 URLs using AI...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "   Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ AI Filtering complete!\n",
            "   Final relevant URLs: 18\n",
            "\n",
            "üìã Sample filtered URLs:\n",
            "   https://www.bms.ac.lk/BSc-(Hons)-Biomedical-Science\n",
            "   https://www.bms.ac.lk/BSc-(Hons)-Food-Science-and-Nutrition\n",
            "   https://www.bms.ac.lk/Bachelor-of-Business-Management(Hons)\n",
            "   https://www.bms.ac.lk/Executive-Certificate-in-Management\n",
            "   https://www.bms.ac.lk/Executive-Diploma-in-Management\n",
            "   https://www.bms.ac.lk/Graduate-Diploma-In-Management\n",
            "   https://www.bms.ac.lk/HD-Biomedical-Science\n",
            "   https://www.bms.ac.lk/HD-Food-Science-and-Nutrition\n",
            "   https://www.bms.ac.lk/Higher-National-Diploma\n",
            "   https://www.bms.ac.lk/International-Foundation-Diploma-(Applied-Science)\n",
            "   ... and 8 more\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Academic keywords for heuristic filtering\n",
        "ACADEMIC_KEYWORDS = [\n",
        "    \"program\", \"programme\", \"course\", \"degree\",\n",
        "    \"undergraduate\", \"postgraduate\", \"graduate\",\n",
        "    \"faculty\", \"department\", \"school\",\n",
        "    \"admission\", \"eligibility\", \"apply\",\n",
        "    \"curriculum\", \"syllabus\",\n",
        "    \"diploma\", \"certificate\", \"bachelor\", \"master\", \"phd\"\n",
        "]\n",
        "\n",
        "NON_ACADEMIC_KEYWORDS = [\n",
        "    \"news\", \"event\", \"staff\", \"research\",\n",
        "    \"library\", \"login\", \"portal\",\n",
        "    \"gallery\", \"download\", \"contact\"\n",
        "]\n",
        "\n",
        "def heuristic_filter(url: str) -> bool:\n",
        "    \"\"\"Quick heuristic check for academic content.\"\"\"\n",
        "    url_lower = url.lower()\n",
        "\n",
        "    # Must contain academic keywords\n",
        "    has_academic = any(kw in url_lower for kw in ACADEMIC_KEYWORDS)\n",
        "\n",
        "    # Must not contain non-academic keywords\n",
        "    has_non_academic = any(kw in url_lower for kw in NON_ACADEMIC_KEYWORDS)\n",
        "\n",
        "    return has_academic and not has_non_academic\n",
        "\n",
        "def ai_filter_batch(urls_batch: List[str], llm) -> List[str]:\n",
        "    \"\"\"Use LLM to filter a batch of URLs.\"\"\"\n",
        "    urls_text = \"\\n\".join(f\"- {url}\" for url in urls_batch)\n",
        "\n",
        "    prompt = f\"\"\"You are filtering URLs for an academic web crawler.\\n\\nSelect URLs that likely contain:\\n- Academic programmes, courses, or degrees\\n- Admission or eligibility information\\n- Faculty or department information\\n- Curriculum or syllabus details\\n\\nReturn ONLY a JSON array of selected URLs.\\nIf none are relevant, return [].\\n\\nURLs to evaluate:\\n{urls_text}\\n\\nJSON array:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt).strip()\n",
        "\n",
        "        # Extract JSON array\n",
        "        match = re.search(r'\\[.*?\\]', response, re.DOTALL)\n",
        "        if match:\n",
        "            selected = json.loads(match.group())\n",
        "            return [url for url in selected if url in urls_batch]\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  AI filter error: {e}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "def filter_urls_with_ai(urls: List[str], llm, batch_size: int, max_workers: int) -> List[str]:\n",
        "    \"\"\"Filter URLs using AI processing.\"\"\"\n",
        "    print(f\"üîç Filtering {len(urls)} URLs using AI...\\n\")\n",
        "    \n",
        "    # AI filtering in batches\n",
        "    # We now use the original 'urls' list directly\n",
        "    batches = [urls[i:i + batch_size] \n",
        "               for i in range(0, len(urls), batch_size)]\n",
        "    \n",
        "    ai_filtered = []\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(ai_filter_batch, batch, llm): i \n",
        "                  for i, batch in enumerate(batches)}\n",
        "        \n",
        "        for future in tqdm(as_completed(futures), \n",
        "                          total=len(futures), \n",
        "                          desc=\"   Processing batches\"):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                if result:\n",
        "                    ai_filtered.extend(result)\n",
        "                time.sleep(0.5)  # Rate limiting\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Batch error: {e}\")\n",
        "    \n",
        "    # Deduplicate and sort the results approved by the AI\n",
        "    final = sorted(list(set(ai_filtered)))\n",
        "    \n",
        "    print(f\"\\n‚úÖ AI Filtering complete!\")\n",
        "    print(f\"   Final relevant URLs: {len(final)}\")\n",
        "    return final\n",
        "\n",
        "# Run the filter\n",
        "filtered_urls = filter_urls_with_ai(\n",
        "    discovered_urls,\n",
        "    llm,\n",
        "    AI_BATCH_SIZE,\n",
        "    AI_MAX_WORKERS\n",
        ")\n",
        "\n",
        "# Display first 10 filtered URLs\n",
        "print(\"\\nüìã Sample filtered URLs:\")\n",
        "for url in filtered_urls[:10]:\n",
        "    print(f\"   {url}\")\n",
        "if len(filtered_urls) > 10:\n",
        "    print(f\"   ... and {len(filtered_urls) - 10} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utOtc6Txh8mf"
      },
      "source": [
        "## üì• Step 6: Download Content\n",
        "\n",
        "Download HTML content from all filtered URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW0ufqbWh8mf",
        "outputId": "5e675594-3451-44ea-805c-21a175a3083c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading content from 18 URLs...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "   Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 28.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Download complete!\n",
            "   Successfully downloaded: 18/18 pages\n",
            "\n",
            "üìä Content statistics:\n",
            "   Total HTML size: 0.83 MB\n",
            "   Average page size: 46.99 KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, Optional\n",
        "\n",
        "def download_url(url: str) -> Optional[str]:\n",
        "    \"\"\"Download HTML content from URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(\n",
        "            url,\n",
        "            timeout=15,\n",
        "            headers={\"User-Agent\": \"Mozilla/5.0 (Academic Crawler)\"}\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Failed to download {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def download_all_content(urls: List[str], max_workers: int) -> Dict[str, str]:\n",
        "    \"\"\"Download HTML content from all URLs in parallel.\"\"\"\n",
        "    print(f\"üì• Downloading content from {len(urls)} URLs...\\n\")\n",
        "\n",
        "    content_map = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(download_url, url): url for url in urls}\n",
        "\n",
        "        for future in tqdm(as_completed(futures),\n",
        "                          total=len(futures),\n",
        "                          desc=\"   Downloading\"):\n",
        "            url = futures[future]\n",
        "            try:\n",
        "                html = future.result()\n",
        "                if html:\n",
        "                    content_map[url] = html\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Error processing {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Download complete!\")\n",
        "    print(f\"   Successfully downloaded: {len(content_map)}/{len(urls)} pages\")\n",
        "    return content_map\n",
        "\n",
        "# Download all content\n",
        "downloaded_content = download_all_content(filtered_urls, MAX_DOWNLOAD_WORKERS)\n",
        "\n",
        "print(f\"\\nüìä Content statistics:\")\n",
        "total_size = sum(len(html) for html in downloaded_content.values())\n",
        "print(f\"   Total HTML size: {total_size / 1024 / 1024:.2f} MB\")\n",
        "print(f\"   Average page size: {total_size / len(downloaded_content) / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzGIeCX_h8mg"
      },
      "source": [
        "## üìù Step 7: Extract Text & Save Files\n",
        "\n",
        "Extract clean text from HTML and save to .txt files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkYvNQath8mg",
        "outputId": "8fe15549-ff5f-480d-f838-702299bff3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Extracting text and saving files...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "   Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:00<00:00, 40.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Text extraction complete!\n",
            "   Files saved: 18\n",
            "   Files skipped (too short): 0\n",
            "   Output directory: /content/drive/MyDrive/academic_content_output/bms_ac_lk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def clean_html_to_text(html: str) -> str:\n",
        "    \"\"\"Extract clean text from HTML.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "\n",
        "    # Remove unwanted elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\",\n",
        "                     \"aside\", \"header\", \"iframe\", \"form\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    # Remove comments\n",
        "    for comment in soup.find_all(string=lambda text:\n",
        "                                 isinstance(text, str) and\n",
        "                                 text.strip().startswith(\"<!--\")):\n",
        "        comment.extract()\n",
        "\n",
        "    # Extract text\n",
        "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "    # Clean up whitespace\n",
        "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "    clean_text = \"\\n\".join(lines)\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "def safe_filename(url: str) -> str:\n",
        "    \"\"\"Generate safe filename from URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    path = parsed.path if parsed.path else \"root\"\n",
        "\n",
        "    # Create readable filename\n",
        "    filename = f\"{parsed.netloc}{path}\"\n",
        "    filename = filename.replace(\"/\", \"_\").replace(\"?\", \"_\")\n",
        "    filename = filename.replace(\"&\", \"_\").replace(\":\", \"_\")\n",
        "    filename = filename.replace(\"=\", \"_\").replace(\".\", \"_\")\n",
        "\n",
        "    # Limit length\n",
        "    if len(filename) > 150:\n",
        "        filename = filename[:150]\n",
        "\n",
        "    return filename + \".txt\"\n",
        "\n",
        "def save_content_to_files(\n",
        "    content_map: Dict[str, str],\n",
        "    output_dir: str,\n",
        "    min_length: int\n",
        ") -> int:\n",
        "    \"\"\"Extract text and save to files.\"\"\"\n",
        "    print(f\"üìù Extracting text and saving files...\\n\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    saved_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    for url, html in tqdm(content_map.items(), desc=\"   Processing\"):\n",
        "        try:\n",
        "            # Extract clean text\n",
        "            clean_text = clean_html_to_text(html)\n",
        "\n",
        "            # Skip if too short\n",
        "            if len(clean_text) < min_length:\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "\n",
        "            # Generate filename\n",
        "            filename = safe_filename(url)\n",
        "            filepath = output_path / filename\n",
        "\n",
        "            # Write to file\n",
        "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"URL: {url}\\n\")\n",
        "                f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "                f.write(clean_text)\n",
        "\n",
        "            saved_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error processing {url}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Text extraction complete!\")\n",
        "    print(f\"   Files saved: {saved_count}\")\n",
        "    print(f\"   Files skipped (too short): {skipped_count}\")\n",
        "    print(f\"   Output directory: {output_dir}\")\n",
        "\n",
        "    return saved_count\n",
        "\n",
        "# Extract and save all content\n",
        "saved_files = save_content_to_files(\n",
        "    downloaded_content,\n",
        "    OUTPUT_DIR,\n",
        "    MIN_TEXT_LENGTH\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
