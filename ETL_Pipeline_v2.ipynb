{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9daa7e",
   "metadata": {},
   "source": [
    "# Educational Data ETL Pipeline with LLM Extraction\n",
    "\n",
    "This notebook implements a production-ready ETL pipeline for extracting structured educational data using LLMs, Pydantic validation, and MongoDB, based on the provided Python code. Each section demonstrates a key part of the pipeline, with code and explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346befca",
   "metadata": {},
   "source": [
    "## 1. Configuration and Directory Setup\n",
    "\n",
    "Define and initialize the configuration for the ETL pipeline, including input, output, and log directories. This section uses a dataclass for centralized configuration and ensures all directories exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Centralized configuration\"\"\"\n",
    "    base_dir: Path = Path(\"/content\" if 'google.colab' in sys.modules else os.getcwd())\n",
    "    input_dir: Path = None\n",
    "    logs_dir: Path = None\n",
    "    output_dir: Path = None\n",
    "    mongodb_uri: str = \"mongodb+srv://ict22006_db_user:gGgnHUqamNcU5jAy@development.ps1jayw.mongodb.net/?appName=development\"\n",
    "    database_name: str = \"edu_platform\"\n",
    "    ollama_base_url: str = \"http://localhost:11434\"\n",
    "    ollama_model: str = \"llama3\"\n",
    "    ollama_temperature: float = 0.1\n",
    "    ollama_timeout: int = 300\n",
    "    chunk_max_chars: int = 3000\n",
    "    chunk_overlap: int = 200\n",
    "    confidence_threshold: float = 0.6\n",
    "    max_retries: int = 2\n",
    "    retry_delay: int = 5\n",
    "    def __post_init__(self):\n",
    "        self.input_dir = self.base_dir / \"bci_lk\"\n",
    "        self.logs_dir = self.base_dir / \"logs\"\n",
    "        self.output_dir = self.base_dir / \"output\"\n",
    "        for dir_path in [self.input_dir, self.logs_dir, self.output_dir]:\n",
    "            dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "config = Config()\n",
    "print(\"Config initialized:\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5d0c1",
   "metadata": {},
   "source": [
    "## 2. Logging Initialization\n",
    "\n",
    "Set up logging to both file and console, and verify that logging output works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_logging():\n",
    "    log_file = config.logs_dir / f'pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(\"etl_notebook\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    if not logger.handlers:\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(console_handler)\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "logger.info(\"Logging initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6eb45a",
   "metadata": {},
   "source": [
    "## 3. Pydantic Schema Definitions\n",
    "\n",
    "Define the Pydantic models for Institution, Program, and ExtractionResult. Validate example data to demonstrate schema usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5b2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pydantic import BaseModel, Field, ConfigDict\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pydantic\"])\n",
    "    from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional, List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "class Institution(BaseModel):\n",
    "    name: str = Field(..., description=\"Official institution name\")\n",
    "    institution_code: Optional[str] = Field(None, description=\"Unique identifier\")\n",
    "    description: Optional[str] = Field(None, description=\"Institution overview\")\n",
    "    type: List[str] = Field(default_factory=list, description=\"Institution types\")\n",
    "    country: str = Field(default=\"Sri Lanka\", description=\"Country\")\n",
    "    website: Optional[str] = Field(None, description=\"Website URL\")\n",
    "    recognition: Optional[Dict[str, Any]] = Field(None, description=\"Accreditation\")\n",
    "    contact_info: Optional[Dict[str, Any]] = Field(None, description=\"Contact details\")\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0-1)\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class Program(BaseModel):\n",
    "    name: str = Field(..., description=\"Program name\")\n",
    "    program_code: Optional[str] = Field(None, description=\"Unique identifier\")\n",
    "    description: Optional[str] = Field(None, description=\"Program overview\")\n",
    "    level: Optional[str] = Field(None, description=\"Academic level\")\n",
    "    duration: Optional[Dict[str, Any]] = Field(None, description=\"Duration details\")\n",
    "    delivery_mode: Optional[List[str]] = Field(None, description=\"Delivery modes\")\n",
    "    fees: Optional[Dict[str, Any]] = Field(None, description=\"Fee structure\")\n",
    "    eligibility: Optional[Dict[str, Any]] = Field(None, description=\"Requirements\")\n",
    "    curriculum_summary: Optional[str] = Field(None, description=\"Curriculum overview\")\n",
    "    specializations: Optional[List[str]] = Field(None, description=\"Specializations\")\n",
    "    extensions: Optional[Dict[str, Any]] = Field(None, description=\"Additional data\")\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0-1)\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class ExtractionResult(BaseModel):\n",
    "    institution: Institution\n",
    "    programs: List[Program] = Field(default_factory=list)\n",
    "    raw_text: str = Field(..., description=\"Original text\")\n",
    "    extraction_timestamp: datetime = Field(default_factory=datetime.now)\n",
    "    source_file: Optional[str] = Field(None, description=\"Source filename\")\n",
    "    model_config = ConfigDict(extra=\"allow\")\n",
    "\n",
    "# Example validation\n",
    "test_inst = Institution(name=\"Test University\", confidence_score=0.95)\n",
    "test_prog = Program(name=\"BSc Computer Science\", confidence_score=0.9)\n",
    "test_result = ExtractionResult(institution=test_inst, programs=[test_prog], raw_text=\"Sample text\")\n",
    "print(\"Validated ExtractionResult:\", test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15c475",
   "metadata": {},
   "source": [
    "## 4. Ollama LLM Client Usage\n",
    "\n",
    "Instantiate the OllamaClient, test the connection, and demonstrate making a sample prompt call to the LLM. This section shows how to interact with the LLM for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"requests\"])\n",
    "    import requests\n",
    "import re, json\n",
    "\n",
    "class OllamaClient:\n",
    "    def __init__(self):\n",
    "        self.base_url = config.ollama_base_url\n",
    "        self.model = config.ollama_model\n",
    "        self.api_url = f\"{self.base_url}/api/generate\"\n",
    "    def call(self, prompt: str) -> dict:\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            json={\n",
    "                \"model\": self.model,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"temperature\": config.ollama_temperature,\n",
    "                \"options\": {\"num_predict\": 4096, \"top_k\": 40, \"top_p\": 0.9}\n",
    "            },\n",
    "            timeout=config.ollama_timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        raw_text = response.json().get(\"response\", \"\")\n",
    "        cleaned = re.sub(r'```json\\\\s*', '', raw_text)\n",
    "        cleaned = re.sub(r'```\\\\s*', '', cleaned)\n",
    "        match = re.search(r'\\{.*\\}', cleaned, re.DOTALL)\n",
    "        return json.loads(match.group(0) if match else cleaned.strip())\n",
    "    def test_connection(self) -> bool:\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "ollama = OllamaClient()\n",
    "print(\"Ollama connection:\", \"OK\" if ollama.test_connection() else \"FAILED\")\n",
    "\n",
    "# Example LLM call (replace with real prompt for actual use)\n",
    "# sample_prompt = \"{\"institution\": {\"name\": \"Test U\", \"confidence_score\": 1.0}, \"programs\": []}\"\n",
    "# print(ollama.call(sample_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f16931e",
   "metadata": {},
   "source": [
    "## 5. Text Chunking and Cleaning\n",
    "\n",
    "Use the TextChunker class to clean and split a sample text into chunks, demonstrating the chunking logic used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class TextChunker:\n",
    "    def __init__(self):\n",
    "        self.max_chars = config.chunk_max_chars\n",
    "        self.overlap = config.chunk_overlap\n",
    "    def chunk_text(self, text: str):\n",
    "        if len(text) <= self.max_chars:\n",
    "            return [text]\n",
    "        chunks, current = [], \"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        for para in paragraphs:\n",
    "            if len(para) > self.max_chars:\n",
    "                sentences = para.split('. ')\n",
    "                for sent in sentences:\n",
    "                    if len(current) + len(sent) + 2 <= self.max_chars:\n",
    "                        current += sent + '. '\n",
    "                    else:\n",
    "                        if current:\n",
    "                            chunks.append(current.strip())\n",
    "                            current = current[-self.overlap:] + sent + '. '\n",
    "                        else:\n",
    "                            current = sent + '. '\n",
    "            else:\n",
    "                if len(current) + len(para) + 2 <= self.max_chars:\n",
    "                    current += para + '\\n\\n'\n",
    "                else:\n",
    "                    if current:\n",
    "                        chunks.append(current.strip())\n",
    "                        current = current[-self.overlap:] + para + '\\n\\n'\n",
    "                    else:\n",
    "                        current = para + '\\n\\n'\n",
    "        if current:\n",
    "            chunks.append(current.strip())\n",
    "        return chunks\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s\\.\\,\\:\\;\\-\\(\\)\\[\\]\\/\\&]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "chunker = TextChunker()\n",
    "sample_text = \"\"\"This is a sample educational program.\\n\\nIt covers multiple topics.\\n\\nThe duration is 3 years.\\n\\n\"\"\"\n",
    "cleaned = chunker.clean_text(sample_text)\n",
    "chunks = chunker.chunk_text(cleaned)\n",
    "print(\"Chunks:\", chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96ac08",
   "metadata": {},
   "source": [
    "## 6. Data Validation and Confidence Scoring\n",
    "\n",
    "Validate a sample extraction result using DataValidator, and normalize confidence scores with ConfidenceScorer. This ensures extracted data meets schema and quality requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ca22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    @staticmethod\n",
    "    def validate_extraction(data: dict, source_file: str = None):\n",
    "        if source_file:\n",
    "            data['source_file'] = source_file\n",
    "        result = ExtractionResult(**data)\n",
    "        logger.info(f\"Validation passed: {result.institution.name}\")\n",
    "        logger.info(f\"Programs: {len(result.programs)}\")\n",
    "        return result\n",
    "    @staticmethod\n",
    "    def validate_confidence(result: ExtractionResult) -> bool:\n",
    "        min_conf = config.confidence_threshold\n",
    "        if result.institution.confidence_score < min_conf:\n",
    "            logger.warning(f\"Low institution confidence: {result.institution.confidence_score}\")\n",
    "            return False\n",
    "        low_progs = [p.name for p in result.programs if p.confidence_score < min_conf]\n",
    "        if low_progs:\n",
    "            logger.warning(f\"Low confidence programs: {len(low_progs)}\")\n",
    "        return True\n",
    "\n",
    "class ConfidenceScorer:\n",
    "    def normalize_confidence(self, result: ExtractionResult) -> ExtractionResult:\n",
    "        result.institution.confidence_score = self._normalize_institution(result.institution)\n",
    "        for program in result.programs:\n",
    "            program.confidence_score = self._normalize_program(program)\n",
    "        return result\n",
    "    def _normalize_institution(self, inst: Institution) -> float:\n",
    "        score = inst.confidence_score\n",
    "        if score < config.confidence_threshold:\n",
    "            score *= 0.9\n",
    "        if not inst.website:\n",
    "            score *= 0.95\n",
    "        if not inst.institution_code:\n",
    "            score *= 0.98\n",
    "        return round(min(score, 1.0), 3)\n",
    "    def _normalize_program(self, prog: Program) -> float:\n",
    "        score = prog.confidence_score\n",
    "        if score < config.confidence_threshold:\n",
    "            score *= 0.85\n",
    "        missing = sum([not prog.level, not prog.duration, not prog.curriculum_summary])\n",
    "        score *= (1.0 - 0.03 * missing)\n",
    "        return round(min(score, 1.0), 3)\n",
    "\n",
    "# Example usage\n",
    "validator = DataValidator()\n",
    "scorer = ConfidenceScorer()\n",
    "\n",
    "sample_data = {\n",
    "    'institution': {'name': 'Test U', 'confidence_score': 0.7},\n",
    "    'programs': [{'name': 'BSc CS', 'confidence_score': 0.5}],\n",
    "    'raw_text': 'test'\n",
    "}\n",
    "result = validator.validate_extraction(sample_data)\n",
    "result = scorer.normalize_confidence(result)\n",
    "validator.validate_confidence(result)\n",
    "print(\"Normalized institution confidence:\", result.institution.confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537dcba8",
   "metadata": {},
   "source": [
    "## 7. MongoDB Writer Operations\n",
    "\n",
    "Connect to MongoDB, insert a sample ExtractionResult, and retrieve database statistics using MongoDBWriter. This demonstrates database integration for storing extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pymongo import MongoClient, errors\n",
    "    from pymongo.server_api import ServerApi\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymongo[srv]\"])\n",
    "    from pymongo import MongoClient, errors\n",
    "    from pymongo.server_api import ServerApi\n",
    "from datetime import datetime\n",
    "\n",
    "class MongoDBWriter:\n",
    "    def __init__(self):\n",
    "        self.client = MongoClient(\n",
    "            config.mongodb_uri,\n",
    "            server_api=ServerApi('1'),\n",
    "            serverSelectionTimeoutMS=5000\n",
    "        )\n",
    "        self.client.admin.command('ping')\n",
    "        self.db = self.client[config.database_name]\n",
    "        self.institutions = self.db.institutions\n",
    "        self.programs = self.db.programs\n",
    "        self.raw_documents = self.db.raw_documents\n",
    "        self.extraction_logs = self.db.extraction_logs\n",
    "        self._create_indexes()\n",
    "    def _create_indexes(self):\n",
    "        self.institutions.create_index(\"name\")\n",
    "        self.institutions.create_index(\n",
    "            \"institution_code\", unique=True, sparse=True,\n",
    "            partialFilterExpression={\"institution_code\": {\"$type\": \"string\"}}\n",
    "        )\n",
    "        self.programs.create_index(\"institution_id\")\n",
    "        self.programs.create_index(\"name\")\n",
    "        self.programs.create_index(\"level\")\n",
    "    def write_extraction(self, result: ExtractionResult) -> dict:\n",
    "        inst_data = result.institution.model_dump()\n",
    "        inst_data['inserted_at'] = datetime.now()\n",
    "        if inst_data.get('institution_code') is None:\n",
    "            inst_data.pop('institution_code', None)\n",
    "        inst_result = self.institutions.insert_one(inst_data)\n",
    "        inst_id = inst_result.inserted_id\n",
    "        prog_ids = []\n",
    "        for program in result.programs:\n",
    "            prog_data = program.model_dump()\n",
    "            prog_data['institution_id'] = inst_id\n",
    "            prog_data['inserted_at'] = datetime.now()\n",
    "            prog_result = self.programs.insert_one(prog_data)\n",
    "            prog_ids.append(prog_result.inserted_id)\n",
    "        raw_doc = {\n",
    "            'institution_id': inst_id,\n",
    "            'raw_text': result.raw_text[:5000],\n",
    "            'source_file': result.source_file,\n",
    "            'extraction_timestamp': result.extraction_timestamp,\n",
    "            'inserted_at': datetime.now()\n",
    "        }\n",
    "        raw_result = self.raw_documents.insert_one(raw_doc)\n",
    "        self.extraction_logs.insert_one({\n",
    "            'institution_id': inst_id,\n",
    "            'source_file': result.source_file,\n",
    "            'programs_count': len(result.programs),\n",
    "            'institution_confidence': result.institution.confidence_score,\n",
    "            'avg_program_confidence': sum(p.confidence_score for p in result.programs) / len(result.programs) if result.programs else 0,\n",
    "            'timestamp': datetime.now(),\n",
    "            'status': 'success'\n",
    "        })\n",
    "        return {'institution_id': inst_id, 'program_ids': prog_ids, 'raw_document_id': raw_result.inserted_id}\n",
    "    def get_statistics(self):\n",
    "        return {\n",
    "            'institutions': self.institutions.count_documents({}),\n",
    "            'programs': self.programs.count_documents({}),\n",
    "            'raw_documents': self.raw_documents.count_documents({}),\n",
    "            'extraction_logs': self.extraction_logs.count_documents({})\n",
    "        }\n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    mongo = MongoDBWriter()\n",
    "    inserted = mongo.write_extraction(result)\n",
    "    print(\"Inserted IDs:\", inserted)\n",
    "    print(\"DB stats:\", mongo.get_statistics())\n",
    "    mongo.close()\n",
    "except Exception as e:\n",
    "    print(\"MongoDB error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeaa406",
   "metadata": {},
   "source": [
    "## 8. Prompt Template Construction\n",
    "\n",
    "Show how to construct system and user prompts for LLM extraction using the provided templates. This ensures the LLM receives clear, schema-aligned instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a structured data extraction engine for educational institutions.\\n\\nCRITICAL RULES:\\n1. Output ONLY valid JSON - no markdown, no code blocks, no explanations\\n2. Follow the JSON schema EXACTLY\\n3. Do NOT guess or hallucinate information\\n4. If data is missing, OMIT the field\\n5. Preserve original wording\\n6. Assign honest confidence_score (0.0-1.0)\\n\\nCONFIDENCE GUIDELINES:\\n- 1.0: Explicitly stated, clear\\n- 0.8-0.9: Clearly stated, minor ambiguity\\n- 0.6-0.7: Implied or partially stated\\n- 0.4-0.5: Inferred from context\\n- 0.0-0.3: Highly uncertain\\n\\nOUTPUT FORMAT:\\n{\\n  \\\"institution\\\": {\\n    \\\"name\\\": \\\"...\\\",\\n    \\\"description\\\": \\\"...\\\",\\n    \\\"type\\\": [\\\"...\\\"],\\n    \\\"country\\\": \\\"Sri Lanka\\\",\\n    \\\"website\\\": \\\"...\\\",\\n    \\\"confidence_score\\\": 0.0-1.0\\n  },\\n  \\\"programs\\\": [\\n    {\\n      \\\"name\\\": \\\"...\\\",\\n      \\\"description\\\": \\\"...\\\",\\n      \\\"level\\\": \\\"...\\\",\\n      \\\"duration\\\": {...},\\n      \\\"curriculum_summary\\\": \\\"...\\\",\\n      \\\"confidence_score\\\": 0.0-1.0\\n    }\\n  ]\\n}\\n\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Extract structured educational data from the following content.\\n\\nSource content:\\n<<<\\n{content}\\n>>>\\n\\nReturn ONLY the JSON object. No explanations. No markdown.\\n\"\"\"\n",
    "\n",
    "# Example prompt construction\n",
    "sample_content = \"Sample university offers a BSc in Computer Science.\"\n",
    "prompt = SYSTEM_PROMPT + USER_PROMPT_TEMPLATE.format(content=sample_content)\n",
    "print(prompt[:400] + \"...\\n[truncated]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532fd61f",
   "metadata": {},
   "source": [
    "## 9. Extraction Pipeline Execution\n",
    "\n",
    "Run the ExtractionPipeline on a sample text file, demonstrating the full ETL process for a single file. This section ties together all previous components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49917070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "class ExtractionPipeline:\n",
    "    def __init__(self):\n",
    "        self.ollama = OllamaClient()\n",
    "        self.chunker = TextChunker()\n",
    "        self.validator = DataValidator()\n",
    "        self.scorer = ConfidenceScorer()\n",
    "        try:\n",
    "            self.mongo = MongoDBWriter()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"MongoDB not available: {e}\")\n",
    "            self.mongo = None\n",
    "    def process_file(self, file_path, save_to_db=True):\n",
    "        logger.info(f\"Processing: {file_path}\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            text = self.chunker.clean_text(text)\n",
    "            chunks = self.chunker.chunk_text(text)\n",
    "            all_programs = []\n",
    "            institution_data = None\n",
    "            for chunk in chunks:\n",
    "                prompt = SYSTEM_PROMPT + USER_PROMPT_TEMPLATE.format(content=chunk)\n",
    "                try:\n",
    "                    response = self.ollama.call(prompt)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"LLM call failed: {e}\")\n",
    "                    continue\n",
    "                if 'institution' in response and institution_data is None:\n",
    "                    institution_data = response['institution']\n",
    "                if 'programs' in response:\n",
    "                    all_programs.extend(response['programs'])\n",
    "            if not institution_data:\n",
    "                logger.error(\"No institution data extracted\")\n",
    "                return None\n",
    "            merged_data = {'institution': institution_data, 'programs': all_programs, 'raw_text': text[:5000]}\n",
    "            result = self.validator.validate_extraction(merged_data, str(file_path))\n",
    "            result = self.scorer.normalize_confidence(result)\n",
    "            self.validator.validate_confidence(result)\n",
    "            if save_to_db and self.mongo:\n",
    "                inserted = self.mongo.write_extraction(result)\n",
    "                logger.info(f\"Saved to MongoDB: {inserted['institution_id']}\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            return None\n",
    "\n",
    "# Example usage (replace 'sample.txt' with a real file path)\n",
    "# pipeline = ExtractionPipeline()\n",
    "# result = pipeline.process_file(config.input_dir / 'sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df296a",
   "metadata": {},
   "source": [
    "## 10. Batch Processing of Files\n",
    "\n",
    "Use BatchProcessor to process multiple files with retry logic, and display a summary of results. This enables robust, large-scale ETL runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645784eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class BatchProcessor:\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.results = {'success': [], 'failed': []}\n",
    "    def process_all(self, files, save_to_db=True):\n",
    "        total = len(files)\n",
    "        logger.info(f\"BATCH PROCESSING: {total} files\")\n",
    "        start_time = time.time()\n",
    "        for idx, file_path in enumerate(files, 1):\n",
    "            logger.info(f\"[{idx}/{total}] Processing: {file_path}\")\n",
    "            result = self._process_with_retry(file_path, save_to_db)\n",
    "            if result:\n",
    "                self.results['success'].append(str(file_path))\n",
    "                logger.info(\"Success\")\n",
    "            else:\n",
    "                self.results['failed'].append(str(file_path))\n",
    "                logger.info(\"Failed\")\n",
    "            success_rate = len(self.results['success']) / idx * 100\n",
    "            logger.info(f\"Progress: {idx}/{total} ({success_rate:.1f}% success)\")\n",
    "        self._print_summary(time.time() - start_time)\n",
    "    def _process_with_retry(self, file_path, save_to_db):\n",
    "        for attempt in range(config.max_retries + 1):\n",
    "            try:\n",
    "                result = self.pipeline.process_file(file_path, save_to_db)\n",
    "                if result:\n",
    "                    return result\n",
    "                if attempt < config.max_retries:\n",
    "                    logger.warning(f\"Retry {attempt + 1}/{config.max_retries}\")\n",
    "                    time.sleep(config.retry_delay)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < config.max_retries:\n",
    "                    time.sleep(config.retry_delay)\n",
    "        return None\n",
    "    def _print_summary(self, elapsed):\n",
    "        total = len(self.results['success']) + len(self.results['failed'])\n",
    "        success_count = len(self.results['success'])\n",
    "        logger.info(f\"BATCH PROCESSING COMPLETE\\nSummary: Total: {total}, Success: {success_count}, Failed: {len(self.results['failed'])}, Time: {elapsed:.1f}s\")\n",
    "        if self.results['failed']:\n",
    "            logger.info(\"Failed files: \" + \", \".join(self.results['failed']))\n",
    "\n",
    "# Example usage (uncomment to run on all .txt files)\n",
    "# pipeline = ExtractionPipeline()\n",
    "# files = list(config.input_dir.glob('*.txt'))\n",
    "# batch = BatchProcessor(pipeline)\n",
    "# batch.process_all(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdc883",
   "metadata": {},
   "source": [
    "## 11. Main Pipeline Execution Example\n",
    "\n",
    "Provide an example of running the main() function, including Ollama connection check and processing a test file. This cell demonstrates a full pipeline run from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e31dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"Educational Data ETL Pipeline Started\")\n",
    "    ollama = OllamaClient()\n",
    "    if not ollama.test_connection():\n",
    "        logger.error(\"Cannot connect to Ollama. Please start the server: ollama serve\")\n",
    "        return\n",
    "    logger.info(\"Ollama connected\")\n",
    "    pipeline = ExtractionPipeline()\n",
    "    files = list(config.input_dir.glob(\"*.txt\"))\n",
    "    logger.info(f\"Found {len(files)} files to process\")\n",
    "    if not files:\n",
    "        logger.error(f\"No files found in {config.input_dir}\")\n",
    "        return\n",
    "    batch_processor = BatchProcessor(pipeline)\n",
    "    if len(files) > 0:\n",
    "        logger.info(\"Processing single file for testing...\")\n",
    "        test_result = pipeline.process_file(files[0], save_to_db=True)\n",
    "        if test_result:\n",
    "            logger.info(f\"Test successful! Institution: {test_result.institution.name}, Programs: {len(test_result.programs)}\")\n",
    "    # To process all files, uncomment:\n",
    "    # batch_processor.process_all(files, save_to_db=True)\n",
    "    logger.info(\"Pipeline execution completed\")\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
